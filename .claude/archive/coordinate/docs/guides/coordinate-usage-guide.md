# /coordinate Command - Usage Guide

**Part 2 of 3** | [Index](coordinate-command-index.md)

This document covers command syntax, phase execution (Phase 0-7), and agent coordination patterns.

---


## Wave-Based Parallel Execution

### Overview

Wave-based execution enables parallel implementation of independent phases, achieving 40-60% time savings compared to sequential execution.

### How It Works

**1. Dependency Analysis**: Parse implementation plan to identify phase dependencies
- Uses `dependency-analyzer.sh` library
- Extracts `dependencies: [N, M]` from each phase
- Builds directed acyclic graph (DAG) of phase relationships

**2. Wave Calculation**: Group phases into waves using Kahn's algorithm
- Wave 1: All phases with no dependencies
- Wave 2: Phases depending only on Wave 1 phases
- Wave N: Phases depending only on previous waves

**3. Parallel Execution**: Execute all phases within a wave simultaneously
- Invoke implementer-coordinator agent for wave orchestration
- Agent spawns implementation-executor agents in parallel (one per phase)
- Wait for all phases in wave to complete before next wave

**4. Wave Checkpointing**: Save state after each wave completes
- Enables resume from wave boundary on interruption
- Tracks wave number, completed phases, pending phases

### Example Wave Execution

```
Plan with 8 phases:
  Phase 1: dependencies: []
  Phase 2: dependencies: []
  Phase 3: dependencies: [1]
  Phase 4: dependencies: [1]
  Phase 5: dependencies: [2]
  Phase 6: dependencies: [3, 4]
  Phase 7: dependencies: [5]
  Phase 8: dependencies: [6, 7]

Wave Calculation Result:
  Wave 1: [Phase 1, Phase 2]          ← 2 phases in parallel (0 dependencies)
  Wave 2: [Phase 3, Phase 4, Phase 5] ← 3 phases in parallel (only depend on Wave 1)
  Wave 3: [Phase 6, Phase 7]          ← 2 phases in parallel (only depend on Waves 1-2)
  Wave 4: [Phase 8]                   ← 1 phase (depends on Wave 3)

Time Savings:
  Sequential: 8 phases × avg_time = 8T
  Wave-based: 4 waves × avg_time = 4T
  Savings: 50% (actual savings depend on phase distribution)
```

### Performance Impact

- **Best case**: 60% time savings (many independent phases)
- **Typical case**: 40-50% time savings (moderate dependencies)
- **Worst case**: 0% savings (fully sequential dependencies)
- **No overhead** for plans with <3 phases (single wave)

### Library Integration

See `.claude/lib/dependency-analyzer.sh` for complete wave calculation implementation.

---

## Plan Naming Convention

Plans created by `/coordinate` follow a descriptive naming pattern to improve repository navigation and discoverability.

### Format

**Pattern**: `{NNN}_{topic_name}_plan.md`

Where:
- **NNN**: Three-digit sequential number (001, 002, 003...)
- **topic_name**: Sanitized workflow description (via `sanitize_topic_name()`)

### Sanitization Algorithm

The topic name is generated by `workflow-initialization.sh` using the `sanitize_topic_name()` function from `topic-utils.sh`:

1. Extract meaningful path components (last 2-3 segments if path provided)
2. Remove filler words ("research", "analyze", "the", "to", "for")
3. Filter stopwords (40+ common English words like "and", "or", "with")
4. Convert to lowercase snake_case
5. Truncate to 50 characters preserving whole words

### Examples

| Workflow Description | Generated Plan Name |
|---------------------|-------------------|
| `fix authentication bug` | `001_fix_authentication_bug_plan.md` |
| `implement user dashboard` | `002_implement_user_dashboard_plan.md` |
| `research /nvim/docs directory` | `003_nvim_docs_directory_plan.md` |
| `refactor state machine for coordinate` | `004_refactor_state_machine_coordinate_plan.md` |

### Why Descriptive Names

- **Context at a glance**: See what the plan is about without opening the file
- **Grep/find friendly**: Search by feature keywords (`find . -name "*auth*_plan.md"`)
- **Consistency**: Matches topic directory naming pattern
- **Navigation**: Improves repository discovery and organization

### Implementation Notes

**NEVER** use generic names like `001_implementation.md` - these provide no context and defeat the purpose of topic-based organization.

The plan path is:
1. Calculated by `workflow-initialization.sh` using `sanitize_topic_name()`
2. Exported as `PLAN_PATH` variable
3. Persisted to workflow state via `state-persistence.sh`
4. Loaded in subsequent bash blocks for verification

The `/coordinate` command trusts this calculated value rather than recalculating or hardcoding paths, following the "single source of truth" principle.

### Related Files

- **Topic naming algorithm**: `.claude/lib/topic-utils.sh` (`sanitize_topic_name()`)
- **Path initialization**: `.claude/lib/workflow-initialization.sh` (`initialize_workflow_paths()`)
- **State persistence**: `.claude/lib/state-persistence.sh` (`append_workflow_state()`, `load_workflow_state()`)
- **Directory protocols**: `.claude/docs/concepts/directory-protocols.md` (topic structure documentation)

### Regression Prevention

A regression test in `.claude/tests/test_orchestration_commands.sh` (Test Suite 5) validates:
- `sanitize_topic_name()` produces expected output
- No hardcoded generic plan paths in `coordinate.md`
- `PLAN_PATH` is properly saved to and loaded from workflow state

Run `./test_orchestration_commands.sh` to verify plan naming implementation.

---

## Performance and Optimization

### Context Usage Targets

**Target**: <30% context usage throughout workflow

**Phase-by-Phase Optimization**:

- **Phase 1 (Research)**: 80-90% reduction via metadata extraction
  - Extract title, summary, key findings only
  - Prune full report content after extraction

- **Phase 2 (Planning)**: 80-90% reduction + pruning research if plan-only workflow
  - Extract plan metadata (phases, complexity, dependencies)
  - Prune research reports after plan creation (if plan-only)

- **Phase 3 (Implementation)**: Aggressive pruning of wave metadata
  - Prune research/planning artifacts after wave calculation
  - Retain only current wave context

- **Phase 4 (Testing)**: Metadata only
  - Pass/fail status retained for potential debugging
  - Prune full test output after summary extraction

- **Phase 5 (Debug)**: Conditional execution
  - Only runs if tests fail
  - Prune test output after debug completion

- **Phase 6 (Documentation)**: Final pruning
  - <30% context usage overall achieved

### File Creation Rate

**Target**: 100% file creation reliability

**Implementation**: Fail-fast verification checkpoints

**Pattern**:
```bash
# Agent invocation
Task { ... }

# MANDATORY VERIFICATION
if [ ! -f "$EXPECTED_PATH" ]; then
  echo "❌ ERROR: Agent failed to create expected file"
  echo "   Expected: $EXPECTED_PATH"
  echo "   Found: File does not exist"
  # ... diagnostic information ...
  exit 1
fi
```

### Progress Streaming

**Format**: Silent PROGRESS: markers at each phase boundary

**Example**:
```
PROGRESS: [Phase 0] - Initialization complete
PROGRESS: [Phase 1] - Invoking 4 research agents in parallel
PROGRESS: [Phase 1] - All research agents completed
PROGRESS: [Phase 2] - Planning phase started
```

**Use Case**: Enables external monitoring without verbose output

---

## Error Handling

### Fail-Fast Philosophy

**Principle**: "One clear execution path, fail fast with full context"

**Key Behaviors**:
- **NO retries**: Single execution attempt per operation
- **NO fallbacks**: If operation fails, report why and exit
- **Clear diagnostics**: Every error shows exactly what failed and why
- **Debugging guidance**: Every error includes steps to diagnose the issue
- **Partial research success**: Continue if ≥50% of parallel agents succeed (Phase 1 only)

**Why Fail-Fast?**
- More predictable behavior (no hidden retry loops)
- Easier to debug (clear failure point, no retry state)
- Easier to improve (fix root cause, not mask with retries)
- Faster feedback (immediate failure notification)

### Error Message Structure

Every error message follows this structure:

```
❌ ERROR: [What failed]
   Expected: [What was supposed to happen]
   Found: [What actually happened]

DIAGNOSTIC INFORMATION:
  - [Specific check that failed]
  - [File system state or error details]
  - [Why this might have happened]

What to check next:
  1. [First debugging step]
  2. [Second debugging step]
  3. [Third debugging step]

Example commands to debug:
  ls -la [path]
  cat [file]
  grep [pattern] [file]
```

### Partial Failure Handling

**Phase 1 (Research)**: Continues if ≥50% of parallel agents succeed

**All Other Phases**: Fail immediately on any agent failure

**Rationale**: Research phase uses multiple parallel agents for redundancy. Other phases have single-agent execution where failure indicates fundamental issues.

---

## Library Dependencies

### Required Libraries

All libraries are required for proper operation. If any library is missing, the command will fail immediately with clear diagnostic information.

**Why All Required?**: Fail-fast philosophy requires all dependencies to be present. Missing libraries indicate configuration issues that should be fixed, not worked around.

### Library List

1. **workflow-detection.sh** - Workflow scope detection and phase execution control
   - `detect_workflow_scope()` - Determine workflow type from description
   - `should_run_phase()` - Check if phase executes for current scope

2. **error-handling.sh** - Error classification and diagnostic message generation
   - `classify_error()` - Classify error type (transient/permanent/fatal)
   - `suggest_recovery()` - Suggest recovery action based on error type
   - `detect_error_type()` - Detect specific error category
   - `extract_location()` - Extract file:line from error message
   - `generate_suggestions()` - Generate error-specific suggestions

3. **checkpoint-utils.sh** - Workflow resume capability and state management
   - `save_checkpoint()` - Save workflow checkpoint for resume
   - `restore_checkpoint()` - Load most recent checkpoint
   - `checkpoint_get_field()` - Extract field from checkpoint
   - `checkpoint_set_field()` - Update field in checkpoint

4. **unified-logger.sh** - Progress tracking and event logging
   - `emit_progress()` - Emit silent progress marker

5. **unified-location-detection.sh** - Topic directory structure creation
   - Topic number allocation
   - Directory structure creation
   - Path calculation

6. **metadata-extraction.sh** - Context reduction via metadata-only passing
   - Report metadata extraction
   - Plan metadata extraction
   - Summary generation

7. **context-pruning.sh** - Context optimization between phases
   - Phase data cleanup
   - Subagent output pruning
   - Context budget management

8. **dependency-analyzer.sh** - Wave-based execution and dependency graph analysis
   - Dependency graph construction
   - Wave calculation (Kahn's algorithm)
   - Topological sorting

### Checkpoint Resume

**Behavior**: Checkpoints saved after Phases 1-4. Auto-resumes from last completed phase on startup.

**Process**:
1. Validate checkpoint exists and is recent
2. Skip completed phases
3. Resume seamlessly from next phase

**See**: [Checkpoint Recovery Pattern](../concepts/patterns/checkpoint-recovery.md) - Schema and implementation details

### Progress Markers

**Format**: `PROGRESS: [Phase N] - [action]`

**Documentation**: See [Orchestration Best Practices → Standardized Progress Markers](./orchestration-best-practices.md#standardized-progress-markers) for format specification, implementation details, parsing examples, and external tool integration.

---

## Usage Examples

### Example 1: Research-Only Workflow

```bash
/coordinate "research GraphQL federation patterns for microservices"
```

**Expected Output**:
```
Phase 0: Initialization complete
PROGRESS: [Phase 0] - Topic directory created

Phase 1: Research
PROGRESS: [Phase 1] - Invoking 4 research agents in parallel
  Agent 1: Architecture patterns
  Agent 2: Implementation examples
  Agent 3: Performance considerations
  Agent 4: Best practices

PROGRESS: [Phase 1] - All research agents completed

✅ Workflow complete: research-only

Artifacts created:
  - specs/042_graphql_federation_patterns_for_microservices/reports/001_architecture_patterns.md
  - specs/042_graphql_federation_patterns_for_microservices/reports/002_implementation_examples.md
  - specs/042_graphql_federation_patterns_for_microservices/reports/003_performance_considerations.md
  - specs/042_graphql_federation_patterns_for_microservices/reports/004_best_practices.md
```

### Example 2: Research-and-Plan Workflow

```bash
/coordinate "research authentication patterns to create OAuth2 implementation plan"
```

**Expected Output**:
```
Phase 0: Initialization complete
PROGRESS: [Phase 0] - Topic directory created

Phase 1: Research
PROGRESS: [Phase 1] - Invoking 4 research agents in parallel
  [... agent details ...]
PROGRESS: [Phase 1] - All research agents completed

Phase 2: Planning
PROGRESS: [Phase 2] - Invoking plan-architect agent
PROGRESS: [Phase 2] - Implementation plan created

✅ Workflow complete: research-and-plan

Artifacts created:
  - specs/043_oauth2_implementation/reports/ (4 reports)
  - specs/043_oauth2_implementation/plans/001_oauth2_implementation_plan.md
```

### Example 3: Full-Implementation Workflow

```bash
/coordinate "implement user profile page with avatar upload"
```

**Expected Output**:
```
Phase 0: Initialization complete
Phase 1: Research complete (4 reports)
Phase 2: Planning complete (1 plan with 6 phases)
Phase 3: Wave-Based Implementation
  Dependency analysis: 3 waves identified
  Wave 1: [Phase 1, Phase 2] ✓
  Wave 2: [Phase 3, Phase 4, Phase 5] ✓
  Wave 3: [Phase 6] ✓
Phase 4: Testing
  Test suite: 23/23 tests passed ✓
Phase 6: Documentation
  Updated: README.md, API.md ✓

✅ Workflow complete: full-implementation

Artifacts created:
  - specs/044_user_profile_with_avatar/reports/ (4 reports)
  - specs/044_user_profile_with_avatar/plans/001_implementation_plan.md
  - specs/044_user_profile_with_avatar/summaries/001_implementation_summary.md
  - src/components/UserProfile.tsx (created)
  - src/api/profileApi.ts (modified)
  - tests/UserProfile.test.tsx (created)
```

### Example 4: Debug-Only Workflow

```bash
/coordinate "fix the infinite loop in the token refresh logic"
```

**Expected Output**:
```
Phase 0: Initialization complete
Phase 1: Root Cause Analysis
  Analyzing: src/auth/tokenRefresh.ts
  Issue identified: Race condition in refresh state
Phase 5: Debug and Fix
  Fix applied: Added mutex lock
  Tests: 5/5 passed ✓

✅ Workflow complete: debug-only

Artifacts created:
  - specs/045_token_refresh_infinite_loop_fix/reports/001_debug_analysis.md
  - src/auth/tokenRefresh.ts (modified)
```

### Example 5: Research-and-Revise Workflow

**Use Case**: Update existing implementation plan based on new research findings

```bash
/coordinate "Revise the plan /home/benjamin/.config/.claude/specs/042_auth/plans/001_auth_plan.md to accommodate recent security research"
```

**Expected Output**:
```
Phase 0: Initialization complete
  - Scope detected: research-and-revise
  - Existing plan: /home/benjamin/.config/.claude/specs/042_auth/plans/001_auth_plan.md
  - EXISTING_PLAN_PATH saved to workflow state ✓

Phase 1: Research (2 agents in parallel)
  - Security best practices analysis
  - Authentication vulnerabilities assessment

Phase 2: Planning (revision-specialist agent invoked)
  - Backup created: 001_auth_plan.md.backup-20251111-120000 ✓
  - Plan revised with security findings
  - Completed phases preserved
  - Revision history updated

✅ Workflow complete: research-and-revise

Artifacts created:
  - specs/042_auth/reports/001_security_analysis.md
  - specs/042_auth/reports/002_vulnerability_assessment.md
  - specs/042_auth/plans/001_auth_plan.md (updated)
  - specs/042_auth/plans/001_auth_plan.md.backup-20251111-120000
```

**Key Differences from Other Workflows**:
- Uses existing topic directory (doesn't create new)
- Invokes revision-specialist agent (not plan-architect)
- Creates timestamped backup before modification
- Preserves completed phases in plan
- Updates revision history section
- Terminal state is "plan" (doesn't proceed to implementation)

**Scope Detection**:
research-and-revise workflow requires both:
1. Revision keyword ("revise", "update plan", "modify plan")
2. Full absolute path to existing plan file

**Path Requirements**:
- MUST be absolute path (starts with `/`)
- MUST match pattern `/specs/NNN_topic/plans/NNN_plan.md`
- Plan file MUST exist before /coordinate is invoked

**Common Errors**:
- "research-and-revise workflow requires existing plan path" → Missing plan path in workflow description
- "Extracted plan path does not exist" → Typo in path or file doesn't exist
- "EXISTING_PLAN_PATH not restored from workflow state" → Bug in Phase 1 (should not occur after Spec 665 fix)

---

## Troubleshooting

### Common Issues

#### Issue 1: "command not found" Errors During Initialization

**Symptom**: `/coordinate` fails with `verify_state_variable: command not found` or `handle_state_error: command not found`

**Root Cause**: Library sourcing order violation - functions called before libraries sourced

**Error Examples**:
```
bash: verify_state_variable: command not found
bash: handle_state_error: command not found
bash: verify_file_created: command not found
```

**Fix**: Verify library sourcing order in coordinate.md

The /coordinate command must source libraries in this order:

```bash
# 1. State machine core (lines 88-105)
source "${LIB_DIR}/workflow-state-machine.sh"
source "${LIB_DIR}/state-persistence.sh"

# 2. Error handling and verification (lines 107-127) - BEFORE any function calls
source "${LIB_DIR}/error-handling.sh"
source "${LIB_DIR}/verification-helpers.sh"

# 3. Additional libraries (line 192+)
source_required_libraries "${REQUIRED_LIBS[@]}"
```

**Verification Commands**:
```bash
# Check sourcing order
grep -n "^source.*error-handling.sh" .claude/commands/coordinate.md
grep -n "^source.*verification-helpers.sh" .claude/commands/coordinate.md
# Both should appear before line 150 (before first function calls)

# Validate with automated test
bash .claude/tests/test_library_sourcing_order.sh
```

**Root Cause Details**:
- Each bash block runs in a separate subprocess
- Functions don't persist across bash block boundaries
- Libraries must be sourced in EVERY block that uses their functions
- Premature function calls (before sourcing) cause "command not found" errors

**Fixed In**: Spec 675 (2025-11-11) - Moved error-handling.sh and verification-helpers.sh sourcing to immediately after state-persistence.sh (lines 107-127), before any verification checkpoints or error handling calls.

**See Also**:
- [Bash Block Execution Model](../concepts/bash-block-execution-model.md#function-availability-and-sourcing-order) - Complete sourcing order documentation
- [Spec 675](../../specs/675_infrastructure_and_the_claude_docs_standards/) - Library sourcing order fix

#### Issue 2: Verification Checkpoint Failures

**Symptom**: "CRITICAL: State file verification failed - variables not written"

**Root Cause Check**:
1. Inspect state file: `cat "$STATE_FILE"`
2. Check if variables present with `export` prefix
3. If variables exist → grep pattern issue (see Spec 644)
4. If variables missing → actual write failure

**Fixed Issues**:
- **Spec 644** (2025-11-10): Grep pattern didn't match export format. Variables were correctly written as `export VAR="value"` but verification checked for `^VAR=` pattern, causing false negatives.

**Solution if variables exist**:
The variables are correctly written. This is a verification pattern bug. Update grep patterns to include `export` prefix:

```bash
# Correct pattern
if grep -q "^export VARIABLE_NAME=" "$STATE_FILE"; then
  echo "✓ Variable verified"
fi
```

**Solution if variables truly missing**:
```bash
# Check append_workflow_state function
source .claude/lib/state-persistence.sh
type append_workflow_state  # Should show function definition

# Check file permissions
ls -la "$STATE_FILE"
df -h "$STATE_FILE"  # Check disk space
```

See [Verification Checkpoint Pattern](../architecture/coordinate-state-management.md#verification-checkpoint-pattern) for complete documentation.

#### Issue 3: State ID File Not Found / State Persistence Failures

**Symptom**: `/coordinate` fails with "State ID file not found" or "CRITICAL: State ID file not created"

**Error Examples**:
```
ERROR: State ID file does not exist
   Expected path: /home/user/.claude/tmp/coordinate_state_id.txt

CRITICAL: State ID file not created at /home/user/.claude/tmp/coordinate_state_id.txt

TROUBLESHOOTING:
  1. Verify init_workflow_state() was called in first bash block
  2. Check STATE_FILE variable was saved to state correctly
  3. Verify workflow ID file exists and contains valid ID
  4. Ensure no premature cleanup of state files
```

**Root Cause**: One of two bugs fixed in Spec 661:
1. **Premature EXIT trap**: Trap in Block 1 deletes state ID file when block exits
2. **Timestamp-based filename**: Discovery pattern fails across subprocess boundaries

**Diagnostic Steps**:

1. **Check if state ID file exists** after Block 1:
```bash
# Run coordinate command, then immediately check:
ls -la "${HOME}/.claude/tmp/coordinate_state_id.txt"

# If missing → EXIT trap fired prematurely (Bug 1)
# If exists → Good, proceed to step 2
```

2. **Check for premature EXIT trap** in Block 1:
```bash
# Search for EXIT trap in first bash block (should NOT exist)
head -200 .claude/commands/coordinate.md | grep "trap.*EXIT.*coordinate_state_id"

# If found → Bug 1 (premature EXIT trap)
# If not found → Pattern 6 correctly implemented
```

3. **Verify fixed semantic filename** used:
```bash
# Check for fixed location (correct pattern)
grep 'COORDINATE_STATE_ID_FILE="${HOME}/.claude/tmp/coordinate_state_id.txt"' \
  .claude/commands/coordinate.md

# If not found → Check for old timestamp pattern (incorrect)
grep 'COORDINATE_STATE_ID_FILE=.*$(date' .claude/commands/coordinate.md
```

4. **Test state persistence** across bash blocks:
```bash
# Run comprehensive test suite
bash .claude/tests/test_coordinate_exit_trap_timing.sh
bash .claude/tests/test_coordinate_bash_block_fixes_integration.sh
```

**Fixed In**: Spec 661 (2025-11-11) - Implemented two critical fixes:

**Fix 1: State ID File Persistence** (Pattern 1 + Pattern 6)
- Changed from timestamp-based to fixed semantic filename
- Removed premature EXIT trap from Block 1
- Moved cleanup to final completion function only

**Fix 2: Library Sourcing Order** (Standard 15)
- Load workflow state BEFORE re-sourcing libraries
- Maintain consistent dependency order in ALL bash blocks
- Add verification checkpoints after library initialization

**Resolution**:

If you encounter this issue, verify both fixes are applied:

```bash
# 1. Verify Pattern 1 (Fixed Semantic Filename)
grep -q 'COORDINATE_STATE_ID_FILE="${HOME}/.claude/tmp/coordinate_state_id.txt"' \
  .claude/commands/coordinate.md && echo "✓ Pattern 1 OK" || echo "✗ Pattern 1 MISSING"

# 2. Verify Pattern 6 (No premature EXIT trap)
! head -200 .claude/commands/coordinate.md | grep -q "trap.*EXIT.*coordinate_state_id" \
  && echo "✓ Pattern 6 OK" || echo "✗ Pattern 6 VIOLATED"

# 3. Verify Standard 15 (Library sourcing order)
bash .claude/tests/test_library_sourcing_order.sh
```

**Why These Patterns Matter**:

- **Pattern 1 (Fixed Semantic Filename)**: Subsequent bash blocks need predictable location to find state ID
- **Pattern 6 (Cleanup on Completion Only)**: EXIT traps fire when bash block exits (subprocess termination), causing premature cleanup
- **Standard 15 (Library Sourcing Order)**: Loading state before sourcing prevents WORKFLOW_SCOPE reset

**Common Mistakes to Avoid**:

❌ **Wrong**: Timestamp-based state ID filename
```bash
COORDINATE_STATE_ID_FILE="${HOME}/.claude/tmp/coordinate_state_id_$(date +%s%N).txt"
# Problem: Subsequent blocks don't know the timestamp
```

✅ **Correct**: Fixed semantic filename
```bash
COORDINATE_STATE_ID_FILE="${HOME}/.claude/tmp/coordinate_state_id.txt"
# Solution: Predictable location for all blocks
```

❌ **Wrong**: EXIT trap in Block 1
```bash
trap 'rm -f "$COORDINATE_STATE_ID_FILE"' EXIT
# Problem: Fires when Block 1 exits, before Block 2 runs
```

✅ **Correct**: Cleanup in final block only
```bash
# Block 1, 2, N-1: NO EXIT traps
# Block N (final): Manual cleanup or trap here
rm -f "$COORDINATE_STATE_ID_FILE" "$STATE_FILE"
```

**See Also**:
- [Bash Block Execution Patterns](#bash-block-execution-patterns) - Complete documentation of all patterns
- [Spec 661](../../specs/661_and_the_standards_in_claude_docs_to_avoid/) - State persistence and library sourcing fixes
- [test_coordinate_exit_trap_timing.sh](../../tests/test_coordinate_exit_trap_timing.sh) - 9 tests validating Pattern 1 + Pattern 6
- [test_coordinate_bash_block_fixes_integration.sh](../../tests/test_coordinate_bash_block_fixes_integration.sh) - 7 integration tests

---

#### Issue 4: Classification Failures

**Symptom**: Error message "CRITICAL ERROR: Comprehensive classification failed"

**Root Cause**: LLM-based classification requires network access and may fail due to:
- No network connection
- API credentials not set
- Classification timeout (default: 30s)
- API service unavailable

**Solutions**:

1. **Check network connectivity**:
   ```bash
   curl -I https://api.anthropic.com  # Should return 200 OK
   ping 8.8.8.8  # Should show successful ping
   ```

2. **Increase timeout** (for slow connections):
   ```bash
   export WORKFLOW_CLASSIFICATION_TIMEOUT=60
   /coordinate "research authentication patterns"
   ```

3. **Check API credentials** (if using external service):
   ```bash
   echo $ANTHROPIC_API_KEY  # Should show key, not empty
   ```

4. **Verify workflow description is clear**:
   ```bash
   # Instead of vague descriptions
   /coordinate "do stuff"  # ❌ Too vague

   # Use specific, clear descriptions
   /coordinate "research authentication patterns and create implementation plan"  # ✓ Clear
   ```

**Note**: regex-only mode was removed in Spec 704 Phase 4 to maintain LLM-only classification with fail-fast error handling.

**Related**: See [LLM Classification Pattern](../concepts/patterns/llm-classification-pattern.md) for complete classification mode documentation.

---

#### Issue 5: Agent Failed to Create Expected File

**Symptoms**:
- Error message: "Agent failed to create expected file"
- Verification checkpoint fails
- Workflow terminates

**Cause**:
- Agent behavioral file missing or incorrect
- Agent misinterpreted instructions
- File system permissions issue
- Path calculation error

**Solution**:
```bash
# Check if agent behavioral file exists
ls -la .claude/agents/research-specialist.md

# Check topic directory permissions
ls -la specs/

# Verify path calculation
echo $REPORT_PATH

# Check agent output for error messages
# (agent output is shown before verification checkpoint)
```

#### Issue 2: Workflow Scope Detection Incorrect

**Symptoms**:
- Wrong phases execute
- Expected phases skipped
- Workflow type mismatch
- "implement <plan-path>" detected as research-and-plan instead of full-implementation

**Root Causes**:
1. **Ambiguous workflow description** - Missing clear keywords
2. **Plan path not recognized** - Path doesn't match `specs/[0-9]+_*/plans/*.md` pattern
3. **Revision pattern takes priority** - "revise...plan" keywords override plan path detection
4. **Outdated scope detection logic** - Bug fixed in Spec 664 (2025-11-11)

**Solution - Test Scope Detection**:
```bash
# Enable debug logging to see detection rationale
export DEBUG_SCOPE_DETECTION=1

# Test scope detection manually
source .claude/lib/workflow-scope-detection.sh
detect_workflow_scope "implement specs/661_auth/plans/001_implementation.md"
# Expected output: full-implementation

# Test revision pattern priority
detect_workflow_scope "revise specs/027_auth/plans/001_plan.md based on feedback"
# Expected output: research-and-revise

# Check detection algorithm (should see priority order comments)
cat .claude/lib/workflow-scope-detection.sh | grep -A 5 "PRIORITY"
```

**Solution - Be More Explicit**:
```bash
# ❌ Ambiguous (will default to research-and-plan)
/coordinate "look at authentication"

# ✅ Explicit research-only
/coordinate "research authentication patterns"

# ✅ Explicit full-implementation (keyword)
/coordinate "implement new authentication feature"

# ✅ Explicit full-implementation (plan path)
/coordinate "implement specs/042_auth/plans/001_oauth.md"

# ✅ Explicit research-and-revise
/coordinate "revise the authentication plan based on security review"
```

**Solution - Check Algorithm Priority**:

Scope detection follows this priority order (as of Spec 664):
1. **Revision patterns** (`revise|update|modify...plan...`)
2. **Plan paths** (`specs/*/plans/*.md`)
3. **Research-only** (`research...` without action keywords)
4. **Explicit implementation** (`implement|execute` keyword)
5. **Other patterns** (`plan`, `debug`, `build feature`, etc.)

If detection seems wrong, check which pattern matched first:
```bash
# View complete detection function
cat .claude/lib/workflow-scope-detection.sh

# Run comprehensive test suite
bash .claude/tests/test_workflow_scope_detection.sh
```

**Fixed in Spec 664** (2025-11-11):
- Plan path detection now recognizes absolute/relative paths and explicit "implement" keyword
- Priority order clarified to handle revision vs implementation correctly
- 20 comprehensive test cases added to prevent regression

---

#### Issue 2a: Revision Workflow Creates New Topic Instead of Using Existing

**Symptoms**:
- Revision workflow creates NEW topic directory (e.g., `662_plans_001_...`)
- Error: "research-and-revise workflow requires /path/to/662_plans_001_.../plans directory but it does not exist"
- Expected EXISTING topic directory not used (e.g., should use `657_topic`)

**Root Cause**:
Path initialization didn't extract topic from provided plan path. This was fixed in Issue #661.

**Solution**:
Ensure you're running latest version with Issue #661 fix:
```bash
# Check if extract_topic_from_plan_path function exists
grep -n "extract_topic_from_plan_path" .claude/lib/workflow-initialization.sh

# Should show function definition around line 78
# If not found, you need to update to latest version
```

**Workaround (if fix not available)**:
Navigate to the existing topic directory before running coordinate:
```bash
cd .claude/specs/657_existing_topic/
/coordinate "Revise the plan ./plans/001_plan.md to accommodate changes"
```

**Correct Workflow Description Format**:
```bash
# Include FULL absolute path to plan file
/coordinate "Revise the plan /home/user/.claude/specs/657_topic/plans/001_plan.md to accommodate new requirements"

# NOT just: "Revise 001_plan.md"
# NOT just: "Update the plan to accommodate..."
