# Non-Interactive Testing Standard

## Metadata

- **Date**: 2025-12-08
- **Version**: 1.0.0
- **Status**: Active
- **Enforcement Level**: ERROR

## Executive Summary

This standard defines requirements for non-interactive testing phases in implementation plans, ensuring all test phases can be executed automatically without manual intervention. Non-interactive testing enables full automation of implementation workflows, CI/CD integration, and wave-based parallel execution through phase dependencies.

**Scope**: Applies to all test phases in implementation plans generated by `/create-plan`, `/lean-plan`, `/repair`, `/debug`, and related planning commands.

**Key Principle**: Every test phase MUST be executable without human interaction, producing programmatic validation results with clear success/failure indicators.

## Required Automation Fields

All test phases MUST include the following metadata fields:

### 1. automation_type

**Type**: String (enum)
**Required**: Yes
**Values**:
- `automated` - Test phase executes without manual intervention
- `manual` - Test phase requires human interaction (PROHIBITED in automated workflows)

**Example**:
```yaml
automation_type: automated
```

### 2. validation_method

**Type**: String (enum)
**Required**: Yes
**Values**:
- `programmatic` - Validation via exit codes, assertions, test frameworks
- `visual` - Validation via human inspection (PROHIBITED in automated workflows)
- `artifact` - Validation via generated artifacts (test reports, coverage data)

**Example**:
```yaml
validation_method: programmatic
```

### 3. skip_allowed

**Type**: Boolean
**Required**: Yes
**Values**:
- `false` - Test phase execution is mandatory
- `true` - Test phase can be skipped (should be rare, requires justification)

**Example**:
```yaml
skip_allowed: false
```

### 4. artifact_outputs

**Type**: Array of strings
**Required**: Yes (can be empty array if no artifacts)
**Description**: List of test artifacts generated during execution

**Example**:
```yaml
artifact_outputs:
  - test-results.xml
  - coverage-report.json
  - test-execution.log
```

## Validation Contracts

### Exit Code Semantics

All test commands MUST follow standard exit code conventions:

- **Exit code 0**: Test execution succeeded, all validations passed
- **Exit code non-zero**: Test execution failed, validation errors occurred

**Example**:
```bash
# Good: Clear success/failure exit codes
pytest tests/ --cov=src --cov-report=json || exit 1

# Bad: No exit code validation
pytest tests/ --cov=src  # continues even if tests fail
```

### Artifact Schema Requirements

Test artifacts MUST be in structured, machine-readable formats:

**Supported Formats**:
- JUnit XML (`test-results.xml`)
- JSON test reports (`test-results.json`)
- Coverage data (JSON, XML, LCOV)
- TAP (Test Anything Protocol)

**Anti-Pattern**: Unstructured text logs without parseable format

### Success Criteria Expressions

Test phases MUST define explicit success criteria using programmatic validation:

**Pattern**:
```bash
# Run tests and validate results
pytest tests/ --cov=src --cov-report=json:coverage.json || exit 1

# Validate coverage threshold (80% minimum)
COVERAGE=$(jq '.totals.percent_covered' coverage.json)
if (( $(echo "$COVERAGE < 80" | bc -l) )); then
    echo "Coverage $COVERAGE% below 80% threshold"
    exit 1
fi
```

**Key Elements**:
1. Test execution with exit code validation
2. Artifact generation (coverage.json)
3. Programmatic threshold validation
4. Clear error messages on failure

## Interactive Anti-Patterns

The following patterns are PROHIBITED in test phases (ERROR-level violations):

### Detection Regex Patterns

1. `\b(manual|manually)\b` - Manual intervention keywords
2. `\bskip\b(?!ped|ping|s)` - Skip directives (excluding "skipped", "skipping", "skips")
3. `\bif needed\b` - Conditional/optional language
4. `\bverify visually\b` - Human inspection requirements
5. `\binspect (the )?output\b` - Manual output inspection
6. `\boptional\b` - Optional execution indicators
7. `\bcheck (the )?results\b` - Manual result checking

### Violation Examples with Corrections

**PROHIBITED**:
```markdown
### Phase 5: Testing
- [ ] Run unit tests
- [ ] Manually verify test output
- [ ] Skip integration tests if needed
- [ ] Inspect coverage report and verify 80% threshold
```

**CORRECTED**:
```markdown
### Phase 5: Testing

**Automation Metadata**:
- automation_type: automated
- validation_method: programmatic
- skip_allowed: false
- artifact_outputs: [test-results.xml, coverage.json]

**Tasks**:
- [ ] Execute unit tests with JUnit XML output: `pytest tests/unit/ --junitxml=test-results.xml || exit 1`
- [ ] Execute integration tests with coverage: `pytest tests/integration/ --cov=src --cov-report=json:coverage.json || exit 1`
- [ ] Validate coverage threshold programmatically: `jq '.totals.percent_covered >= 80' coverage.json | grep -q true || exit 1`
```

### Ambiguous Pattern Guidance

Some patterns are context-dependent and require judgment:

**Acceptable Uses**:
- "verify" in programmatic context: "Verify exit code is 0"
- "check" in programmatic context: "Check test artifacts exist"
- "manual" in documentation context: "This replaces manual testing"

**Unacceptable Uses**:
- "verify" requiring human judgment: "Verify output looks correct"
- "check" without automation: "Check if tests passed"
- "manual" as execution directive: "Manually run tests"

## Integration Points

### Plan-Architect Agent Integration

The `plan-architect` agent MUST generate test phases with automation metadata fields. See [Plan-Architect Guidelines](./../../../agents/plan-architect.md) for implementation details.

**Required Behavior**:
1. Extract automation requirements from standards context
2. Generate test phases with all required metadata fields
3. Validate test phase content against anti-pattern regex
4. Include programmatic validation commands with exit code checking

### format_standards_for_prompt() Integration

Planning commands inject this standard via `format_standards_for_prompt()` function. See [Command Authoring Standards](./command-authoring.md#plan-metadata-standard-integration) for implementation pattern.

**Injection Points**:
- `/create-plan` - Standard software implementation plans
- `/lean-plan` - Lean theorem proving plans (compiler-based validation)
- `/repair` - Error repair plans with validation phases
- `/debug` - Debug workflow plans with test phases

**Implementation**:
```bash
# Extract key requirements for agent context
TESTING_STANDARDS=$(format_standards_for_prompt \
    --non-interactive-testing="$CLAUDE_DOCS/reference/standards/non-interactive-testing-standard.md")

# Include in agent delegation
"Workflow-Specific Context:
$TESTING_STANDARDS

Generate test phases with automation metadata fields..."
```

### Pre-Commit Validation

The `validate-non-interactive-tests.sh` validator runs during pre-commit hooks, blocking commits with interactive anti-patterns. See [Enforcement Mechanisms](./enforcement-mechanisms.md) for complete enforcement details.

**Validator Behavior**:
- Scans test phases in implementation plans
- Detects interactive anti-patterns via regex matching
- Reports violations with line numbers and context
- Exit code 1 (ERROR) if violations found, exit code 0 if clean

**Bypass Procedure**:
```bash
# Bypass validator (document justification in commit message)
git commit --no-verify -m "feat: add plan with manual testing phase

Justification for manual testing:
- Integration requires physical hardware not available in CI
- Plan includes automated phases for all software-testable components
"
```

## Enforcement Mechanisms

### Automated Validation

**Validator**: `validate-non-interactive-tests.sh`
**Severity**: ERROR
**Scope**: All test phases in `.claude/specs/*/plans/*.md` files
**Trigger**: Pre-commit hooks, CI validation, `/test` command

**Validation Flow**:
1. Extract test phase sections from implementation plan
2. Apply anti-pattern regex patterns to phase content
3. Generate violation report with file path, line number, pattern match
4. Return exit code 1 if ERROR-level violations detected

### Standards Compliance Checks

Run comprehensive validation via:
```bash
# Validate all plans for non-interactive testing compliance
bash .claude/scripts/validate-all-standards.sh --non-interactive-tests

# Validate specific plan file
bash .claude/scripts/validate-non-interactive-tests.sh --file path/to/plan.md

# Validate git staged files only (pre-commit mode)
bash .claude/scripts/validate-non-interactive-tests.sh --staged
```

### Enforcement Levels

| Violation Type | Severity | Action |
|---------------|----------|--------|
| Explicit interactive directives ("manually verify", "skip if needed") | ERROR | Block commit, require correction |
| Ambiguous patterns without automation context | WARNING | Allow commit, suggest improvement |
| Missing automation metadata fields | ERROR | Block commit, require fields |
| Programmatic validation with clear exit codes | PASS | Allow commit |

## Practical Examples

### Example 1: Automated Unit Tests

```markdown
### Phase 3: Unit Testing

**Automation Metadata**:
- automation_type: automated
- validation_method: programmatic
- skip_allowed: false
- artifact_outputs: [test-results.xml, coverage.json]

**Tasks**:
- [ ] Execute unit test suite: `pytest tests/unit/ --junitxml=test-results.xml -v || exit 1`
- [ ] Generate coverage report: `pytest tests/unit/ --cov=src --cov-report=json:coverage.json --cov-report=html:htmlcov/ || exit 1`
- [ ] Validate coverage threshold (85% minimum): `jq '.totals.percent_covered >= 85' coverage.json | grep -q true || { echo "Coverage below 85% threshold"; exit 1; }`
- [ ] Validate no skipped tests: `xmllint --xpath 'count(//testcase[@status="skipped"])' test-results.xml | grep -q '^0$' || { echo "Skipped tests detected"; exit 1; }`
```

**Why This Works**:
- Clear automation metadata at phase level
- Programmatic validation with exit codes
- Structured artifacts (JUnit XML, JSON coverage)
- Explicit success criteria with threshold validation

### Example 2: Lean Proof Validation

```markdown
### Phase 4: Proof Validation

**Automation Metadata**:
- automation_type: automated
- validation_method: programmatic
- skip_allowed: false
- artifact_outputs: [build.log, proof-verification.txt]

**Tasks**:
- [ ] Build Lean project with proof verification: `cd project/ && lake build 2>&1 | tee build.log || exit 1`
- [ ] Verify no sorry markers remain: `rg -t lean 'sorry' src/ && { echo "Incomplete proofs detected"; exit 1; } || echo "All proofs complete"`
- [ ] Validate theorem compilation: `lake build ProofTheorem.lean || { echo "Theorem compilation failed"; exit 1; }`
- [ ] Extract verification status: `grep -q "building ProofTheorem" build.log && echo "Proof verified" > proof-verification.txt`
```

**Why This Works**:
- Uses Lean compiler as test oracle (automated validation)
- Exit code checking for build success/failure
- Programmatic sorry marker detection
- Generated artifacts for verification audit trail

### Example 3: Integration Test Orchestration

```markdown
### Phase 5: Integration Testing

**Automation Metadata**:
- automation_type: automated
- validation_method: artifact
- skip_allowed: false
- artifact_outputs: [integration-results.json, service-logs.txt]

**Tasks**:
- [ ] Start test services: `docker-compose -f docker-compose.test.yml up -d && sleep 5`
- [ ] Execute integration test suite: `pytest tests/integration/ --json-report --json-report-file=integration-results.json || { docker-compose -f docker-compose.test.yml down; exit 1; }`
- [ ] Validate API endpoints: `curl -f http://localhost:8080/health || { echo "Health check failed"; exit 1; }`
- [ ] Collect service logs: `docker-compose -f docker-compose.test.yml logs > service-logs.txt`
- [ ] Teardown test services: `docker-compose -f docker-compose.test.yml down`
```

**Why This Works**:
- Complete orchestration automation (startup, test, teardown)
- JSON artifact generation for test results
- HTTP health check with exit code validation
- Service log collection for debugging

## Cross-References

**Related Standards**:
- [Testing Protocols](./../../../docs/reference/standards/testing-protocols.md) - Complete test discovery, patterns, coverage requirements
- [Plan Metadata Standard](./plan-metadata-standard.md) - Automation workflow extension fields
- [Command Authoring Standards](./command-authoring.md) - Standards injection patterns

**Integration Documentation**:
- [Plan-Architect Agent Guidelines](./../../../agents/plan-architect.md) - Test phase generation behavioral requirements
- [Enforcement Mechanisms](./enforcement-mechanisms.md) - Validator integration and bypass procedures
- [Command Patterns Quick Reference](./../../../docs/reference/command-patterns-quick-reference.md) - Copy-paste test phase templates

**Related Commands**:
- `/create-plan` - Generates implementation plans with non-interactive test phases
- `/lean-plan` - Generates Lean plans with compiler-based validation phases
- `/implement` - Executes test phases according to automation metadata
- `/test` - Runs test suite with coverage validation
