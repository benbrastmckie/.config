# Testing Requirements for Hybrid Workflow Classification System

## Metadata
- **Date**: 2025-11-12
- **Agent**: research-specialist
- **Topic**: Testing Strategy for Hybrid Classification System
- **Report Type**: codebase analysis
- **Implementation**: `/home/benjamin/.config/.claude/specs/670_workflow_classification_improvement/plans/001_hybrid_classification_implementation.md`
- **Status**: Phase 1-2 Complete (Unit & Integration Tests Implemented)

## Executive Summary

The hybrid workflow classification system has comprehensive test coverage with 37 unit tests (test_llm_classifier.sh), 31 integration tests (test_scope_detection.sh), 42 A/B comparison tests (test_scope_detection_ab.sh), and 6 manual E2E tests. Current test pass rate is 97% (65/67 automated tests passing, 2 skipped for manual LLM integration). Critical gaps include: (1) no real LLM integration tests, (2) limited performance benchmarking, (3) minimal error recovery scenario testing. Recommended additions: automated LLM mock framework, performance regression suite, and failure injection tests.

## Findings

### 1. Existing Test Coverage Analysis

#### 1.1 Unit Tests (test_llm_classifier.sh)
**File**: `/home/benjamin/.config/.claude/tests/test_llm_classifier.sh` (442 lines)
**Coverage**: 37 tests across 7 sections
**Pass Rate**: 35/37 (94.6%, 2 skipped for manual integration)

**Test Sections**:
1. **Input Validation** (5 tests): Empty input, valid input, long descriptions (500+ chars), special characters, Unicode
2. **JSON Building** (3 tests): Structure validation, valid scopes array, description preservation
3. **Response Parsing** (9 tests): Valid responses, missing fields (scope/confidence/reasoning), invalid scope values, invalid confidence ranges, malformed JSON, empty responses
4. **Confidence Threshold** (4 tests): Above threshold (0.95), below threshold (0.5), exact threshold (0.7), environment variable override
5. **Logging** (5 tests): Debug mode on/off, error logging, structured logging format
6. **Environment Configuration** (4 tests): Confidence threshold, timeout, debug mode configuration
7. **Error Handling** (3 tests): Empty input rejection, timeout mechanism (skipped), invalid JSON handling (skipped)

**Key Strengths**:
- Comprehensive input validation (empty, long, special chars, Unicode)
- Thorough JSON parsing edge cases (9 tests)
- Confidence threshold boundary testing
- Mock-based testing (no real LLM calls)

**Identified Gaps**:
- Real LLM integration tests skipped (2 tests)
- Timeout behavior not validated (requires integration test)
- No performance benchmarks (latency, throughput)

#### 1.2 Integration Tests (test_scope_detection.sh)
**File**: `/home/benjamin/.config/.claude/tests/test_scope_detection.sh` (560 lines)
**Coverage**: 31 tests across 8 sections
**Pass Rate**: 30/31 (96.8%, 1 skipped for edge case priority)

**Test Sections**:
1. **Regex-Only Mode** (5 tests): Plan path detection, plan keyword, research-only pattern, revise pattern, implement keyword
2. **Hybrid Mode** (3 tests): Default behavior, LLM success path (mocked), fallback on timeout
3. **LLM-Only Mode** (2 tests): Success path, fail-fast on error
4. **Mode Configuration** (3 tests): Valid modes (hybrid/llm-only/regex-only), invalid mode handling, environment variable switching
5. **Edge Cases** (5 tests): Empty input, quoted keywords, negation, special characters, malformed input
6. **Backward Compatibility** (5 tests): Function signature unchanged, existing callers work, default scope fallback, error handling, integration with workflow-detection.sh
7. **workflow-detection.sh Integration** (2 tests): Library sourcing, function availability
8. **Comprehensive Edge Cases** (6 tests): Quoted keywords (3 patterns), negation (3 patterns), multiple actions (1 pattern, skipped), long descriptions, special characters (4 patterns), empty/malformed input

**Key Strengths**:
- All three modes tested (hybrid, llm-only, regex-only)
- Backward compatibility verified (detect_workflow_scope unchanged)
- Edge case coverage (quoted keywords, negation, special chars)
- Fallback scenarios tested

**Identified Gaps**:
- Multiple action priority not tested (1 skip: "research X, plan Y, and implement Z")
- LLM confidence levels not tested in integration context
- Hybrid mode only uses mocked LLM responses

#### 1.3 A/B Comparison Tests (test_scope_detection_ab.sh)
**File**: `/home/benjamin/.config/.claude/tests/test_scope_detection_ab.sh` (305 lines)
**Coverage**: 42 test cases comparing LLM vs regex classification
**Pass Rate**: 41/42 (97.6%, 1 disagreement documented)

**Test Case Categories**:
1. **Straightforward Cases** (7 tests): research-and-plan, full-implementation, research-and-revise, research-only, plan keyword, test workflow, debug-only
2. **Edge Cases** (3 tests): Discussing workflow types (original issue), analyze command implementation, investigate workflow failure
3. **Quoted Keywords** (2 tests): "implement" command, "revise" function
4. **Negation** (2 tests): Don't revise (create new), research alternatives (not implement)
5. **Multiple Actions** (2 tests): Research + plan + implement, implement + test
6. **Ambiguous Intent** (3 tests): Look into output, check results, review documentation
7. **Complex Descriptions** (2 tests): Multi-step research/analysis/planning, OAuth2 implementation
8. **Scope Variations** (21 tests):
   - Research-and-plan (6 variations)
   - Research-and-revise (3 variations)
   - Full-implementation (3 variations)
   - Debug (3 variations)
   - Research-only (3 variations)
   - Additional edge cases (3 variations)

**Key Strengths**:
- Real-world test descriptions (50+ workflow descriptions)
- Human-validated expected classifications
- Agreement rate measurement (97% pass rate)
- Disagreement tracking (1 documented case)
- Coverage of original issue edge case ("research the research-and-revise workflow")

**Identified Gaps**:
- A/B tests currently use regex-only mode (no real LLM comparison yet)
- Performance comparison not included (LLM vs regex latency)
- Confidence score distribution not analyzed

#### 1.4 Manual E2E Tests (manual_e2e_hybrid_classification.sh)
**File**: `/home/benjamin/.config/.claude/tests/manual_e2e_hybrid_classification.sh` (196 lines)
**Coverage**: 6 manual test cases
**Execution Status**: 3/6 verified in regex-only mode, 3/6 pending LLM integration

**Test Cases**:
1. **Problematic Edge Case** (pending LLM): "research the research-and-revise workflow misclassification issue" â†’ expected research-and-plan
2. **Normal Case** (verified): "research authentication patterns and create implementation plan" â†’ research-and-plan
3. **Revision Case** (verified): "Revise the plan at specs/042/plans/001.md based on new requirements" â†’ research-and-revise
4. **Fallback Case** (verified): Force timeout (WORKFLOW_CLASSIFICATION_TIMEOUT=0) â†’ regex fallback transparent
5. **LLM-Only Fail-Fast** (pending LLM): Verify error message when LLM unavailable
6. **Mode Switching** (verified): Both hybrid and regex-only return same result for "implement user authentication"

**Key Strengths**:
- Real /coordinate workflow integration
- Fallback transparency validated
- Mode switching verified
- Debug logging verification included

**Identified Gaps**:
- No real LLM integration (3/6 tests pending)
- No latency measurements
- No concurrent workflow testing
- No production load testing

### 2. Critical Test Scenarios Needed

#### 2.1 LLM Classification Success Cases

**Priority**: High
**Current Status**: Mocked in unit tests, not tested with real LLM

**Required Tests**:
1. **Intent Detection Accuracy**:
   - Test: "research the research-and-revise workflow" â†’ research-and-plan (not research-and-revise)
   - Test: "analyze the implement command" â†’ research-and-plan (not full-implementation)
   - Test: "investigate coordinate output" â†’ research-and-plan (not full-implementation)
   - **Expected**: LLM correctly identifies user intent (not keyword matching)

2. **Confidence Scoring**:
   - Test: Clear intent ("implement auth") â†’ confidence >0.9
   - Test: Ambiguous intent ("look into X") â†’ confidence 0.6-0.8
   - Test: Highly ambiguous ("check that thing") â†’ confidence <0.5 â†’ triggers fallback
   - **Expected**: Confidence correlates with classification certainty

3. **Semantic Understanding**:
   - Test: Quoted keywords ("research the 'implement' command") â†’ research-and-plan (quotes indicate discussion, not action)
   - Test: Negation ("don't revise the plan, create new") â†’ research-and-plan (negation changes intent)
   - Test: Multiple actions ("research X, plan Y, implement Z") â†’ prioritize final action (full-implementation)
   - **Expected**: LLM understands context, not just keywords

**Implementation Approach**:
```bash
# Create test fixture with real LLM responses
# Store in .claude/tests/fixtures/llm_classification_responses.json

{
  "intent_detection": [
    {
      "input": "research the research-and-revise workflow",
      "expected_scope": "research-and-plan",
      "expected_confidence": 0.85,
      "reasoning": "User wants to research (learn about) a workflow type, not execute revision"
    }
  ],
  "confidence_scoring": [...],
  "semantic_understanding": [...]
}

# Test runner sources fixture, invokes real LLM, compares results
```

#### 2.2 Automatic Fallback Scenarios

**Priority**: High
**Current Status**: Partially tested (timeout scenario verified in integration tests)

**Required Tests**:
1. **LLM Timeout Fallback**:
   - Test: Set WORKFLOW_CLASSIFICATION_TIMEOUT=0 â†’ immediate timeout
   - Test: Set WORKFLOW_CLASSIFICATION_TIMEOUT=1 â†’ short timeout (verify fallback within 1s)
   - **Expected**: Fallback to regex transparent (no user-visible errors), workflow continues normally

2. **Low Confidence Fallback**:
   - Test: Mock LLM response with confidence 0.3 â†’ triggers fallback
   - Test: Mock LLM response with confidence 0.69 â†’ triggers fallback (below 0.7 threshold)
   - Test: Mock LLM response with confidence 0.7 â†’ no fallback (at threshold)
   - **Expected**: Fallback occurs at correct threshold, logs show "regex-fallback" method

3. **API Error Fallback**:
   - Test: Mock Task tool failure (no response file created)
   - Test: Mock Task tool returns invalid JSON
   - Test: Mock Task tool returns empty response
   - **Expected**: All error conditions trigger regex fallback gracefully

4. **Invalid Response Handling**:
   - Test: Missing required fields (scope, confidence, reasoning)
   - Test: Invalid scope value ("unknown-scope")
   - Test: Invalid confidence value (1.5, -0.5, "abc")
   - **Expected**: Parsing fails, triggers regex fallback with error logging

**Test Data Fixtures**:
```bash
# .claude/tests/fixtures/invalid_llm_responses.sh
INVALID_RESPONSES=(
  '{"scope": "research-and-plan"}'  # Missing confidence
  '{"scope": "invalid-type", "confidence": 0.9, "reasoning": "test"}'  # Invalid scope
  '{"scope": "research-and-plan", "confidence": 1.5, "reasoning": "test"}'  # Confidence out of range
  '{"malformed": true'  # Malformed JSON
  ''  # Empty response
)
```

#### 2.3 Performance Characteristics

**Priority**: Medium
**Current Status**: No performance tests exist

**Required Benchmarks**:
1. **Latency Measurements**:
   - **p50 Latency**: <300ms for LLM classification (goal from plan)
   - **p95 Latency**: <600ms for LLM classification
   - **p99 Latency**: <1000ms for LLM classification
   - **Regex Baseline**: <1ms (for comparison)
   - **Fallback Overhead**: Measure total time for LLM timeout â†’ regex fallback

2. **Throughput Testing**:
   - **Sequential**: 10 classifications in a row (measure total time, detect memory leaks)
   - **Concurrent**: 5 concurrent classifications (test file locking, cleanup)
   - **Expected**: No degradation over time, no resource leaks

3. **Resource Usage**:
   - **Memory**: Measure RSS before/after 100 classifications
   - **File Handles**: Verify temp files cleaned up (no fd leaks)
   - **CPU**: Profile bash execution time (not LLM wait time)

**Test Implementation**:
```bash
#!/usr/bin/env bash
# .claude/tests/bench_workflow_classification.sh

# Benchmark latency
for i in {1..100}; do
  start=$(date +%s%3N)
  result=$(detect_workflow_scope "research auth patterns")
  end=$(date +%s%3N)
  echo "$((end - start))ms" >> latencies.txt
done

# Calculate percentiles
sort -n latencies.txt | awk '
  {latency[NR] = $1}
  END {
    print "p50:", latency[int(NR*0.5)]
    print "p95:", latency[int(NR*0.95)]
    print "p99:", latency[int(NR*0.99)]
  }
'
```

#### 2.4 Integration with Workflow Detection

**Priority**: Medium
**Current Status**: Basic integration tested (workflow-detection.sh sources unified library)

**Required Tests**:
1. **/coordinate Workflow Integration**:
   - Test: Full /coordinate workflow with hybrid mode enabled
   - Test: Verify WORKFLOW_SCOPE environment variable set correctly
   - Test: Verify state machine receives correct scope
   - **Expected**: /coordinate workflow completes successfully with hybrid classification

2. **/supervise Workflow Integration**:
   - Test: /supervise sources workflow-detection.sh which sources workflow-scope-detection.sh
   - Test: Verify should_run_phase() function uses unified classification
   - **Expected**: /supervise workflow uses hybrid classification transparently

3. **State Machine Integration**:
   - Test: sm_init() function (workflow-state-machine.sh:89) calls detect_workflow_scope()
   - Test: Verify state machine maps scope correctly (research-only â†’ STATE_RESEARCH, etc.)
   - Test: Verify backward compatibility (function signature unchanged)
   - **Expected**: State machine initializes correctly with hybrid classification

4. **Error Propagation**:
   - Test: detect_workflow_scope() error (empty input) â†’ workflow receives default scope
   - Test: LLM-only mode failure â†’ workflow receives error message and default scope
   - **Expected**: Errors don't crash workflow, clear error messages logged

### 3. Test Data Requirements and Fixtures

#### 3.1 LLM Response Fixtures

**Location**: `/home/benjamin/.config/.claude/tests/fixtures/llm_responses/`

**Required Fixtures**:
1. **Valid Responses** (valid_responses.json):
   ```json
   {
     "high_confidence_research_and_plan": {
       "scope": "research-and-plan",
       "confidence": 0.95,
       "reasoning": "User wants to research patterns and create implementation plan"
     },
     "low_confidence_ambiguous": {
       "scope": "research-and-plan",
       "confidence": 0.45,
       "reasoning": "Intent unclear, defaulting to research-and-plan"
     },
     "edge_case_discussed_workflow": {
       "scope": "research-and-plan",
       "confidence": 0.88,
       "reasoning": "User discussing workflow types, not requesting revision"
     }
   }
   ```

2. **Invalid Responses** (invalid_responses.json):
   ```json
   {
     "missing_scope": {"confidence": 0.9, "reasoning": "test"},
     "missing_confidence": {"scope": "research-and-plan", "reasoning": "test"},
     "missing_reasoning": {"scope": "research-and-plan", "confidence": 0.9},
     "invalid_scope": {"scope": "unknown-type", "confidence": 0.9, "reasoning": "test"},
     "confidence_out_of_range": {"scope": "research-and-plan", "confidence": 1.5, "reasoning": "test"},
     "confidence_negative": {"scope": "research-and-plan", "confidence": -0.5, "reasoning": "test"},
     "malformed_json": "{\"scope\": \"research-and-plan\""
   }
   ```

3. **Edge Case Responses** (edge_cases.json):
   ```json
   {
     "quoted_keywords": {
       "input": "research the 'implement' command",
       "response": {
         "scope": "research-and-plan",
         "confidence": 0.92,
         "reasoning": "Quotes indicate discussion of 'implement' keyword, not action"
       }
     },
     "negation": {
       "input": "don't revise the plan, create new",
       "response": {
         "scope": "research-and-plan",
         "confidence": 0.87,
         "reasoning": "Negation of 'revise', user wants to create new plan"
       }
     }
   }
   ```

#### 3.2 Workflow Description Test Dataset

**Location**: `/home/benjamin/.config/.claude/tests/fixtures/workflow_descriptions.txt`

**Dataset Structure**:
```
# Format: description|expected_scope|category|notes

# Straightforward cases
research authentication patterns and create plan|research-and-plan|normal|basic research-and-plan
implement user authentication|full-implementation|normal|explicit implement keyword
revise specs/042/plans/001.md based on feedback|research-and-revise|normal|revision with path

# Edge cases from issue #670
research the research-and-revise workflow|research-and-plan|edge_case|discussed workflow type
analyze the implement command|research-and-plan|edge_case|discussed command name
investigate coordinate output|research-and-plan|edge_case|ambiguous action

# Semantic understanding tests
research the 'implement' command source|research-and-plan|semantic|quoted keyword
don't revise the plan, create new|research-and-plan|semantic|negation
research X, plan Y, and implement Z|full-implementation|semantic|multiple actions

# Ambiguous intent (confidence boundary testing)
look into the auth system|research-and-plan|ambiguous|vague action
check test results|research-and-plan|ambiguous|vague action
review documentation|research-and-plan|ambiguous|vague action

# Long descriptions (500+ characters)
research authentication patterns in the codebase [...500 chars...]|research-and-plan|long|complex description
```

**Dataset Size**: 50+ workflow descriptions covering all scope types and edge cases

#### 3.3 Mock LLM Response Generator

**Location**: `/home/benjamin/.config/.claude/tests/mocks/mock_llm_classifier.sh`

**Purpose**: Replace real LLM invocation with fixture-based responses for deterministic testing

**Implementation**:
```bash
#!/usr/bin/env bash
# Mock LLM classifier for testing

mock_classify_workflow_llm() {
  local workflow_description="$1"
  local fixture_file="${FIXTURE_DIR}/llm_responses/valid_responses.json"

  # Map description to fixture response
  case "$workflow_description" in
    "research the research-and-revise workflow"*)
      jq '.edge_case_discussed_workflow' "$fixture_file"
      return 0
      ;;
    "implement"*)
      jq '.high_confidence_full_implementation' "$fixture_file"
      return 0
      ;;
    *)
      # Default response
      jq '.high_confidence_research_and_plan' "$fixture_file"
      return 0
      ;;
  esac
}

export -f mock_classify_workflow_llm
```

### 4. Expected Outputs and Verification Methods

#### 4.1 Classification Output Validation

**Function**: `detect_workflow_scope()`
**Output Format**: String (one of: research-only, research-and-plan, research-and-revise, full-implementation, debug-only)

**Verification Methods**:
1. **Exact Match Verification**:
   ```bash
   result=$(detect_workflow_scope "research auth patterns")
   [ "$result" = "research-and-plan" ] || fail "Expected research-and-plan, got $result"
   ```

2. **Valid Scope Verification**:
   ```bash
   VALID_SCOPES=("research-only" "research-and-plan" "research-and-revise" "full-implementation" "debug-only")
   result=$(detect_workflow_scope "$description")
   is_valid=0
   for scope in "${VALID_SCOPES[@]}"; do
     [ "$result" = "$scope" ] && is_valid=1
   done
   [ $is_valid -eq 1 ] || fail "Invalid scope returned: $result"
   ```

3. **Classification Method Verification** (debug mode):
   ```bash
   DEBUG_SCOPE_DETECTION=1 result=$(detect_workflow_scope "$description" 2>&1)
   if echo "$result" | grep -q "method=llm"; then
     echo "âœ“ LLM classification used"
   elif echo "$result" | grep -q "method=regex-fallback"; then
     echo "âœ“ Regex fallback triggered"
   fi
   ```

#### 4.2 LLM Response Validation

**Function**: `classify_workflow_llm()`
**Output Format**: JSON object with scope, confidence, reasoning

**Verification Methods**:
1. **JSON Structure Validation**:
   ```bash
   result=$(classify_workflow_llm "$description")
   echo "$result" | jq -e '.scope' >/dev/null || fail "Missing scope field"
   echo "$result" | jq -e '.confidence' >/dev/null || fail "Missing confidence field"
   echo "$result" | jq -e '.reasoning' >/dev/null || fail "Missing reasoning field"
   ```

2. **Field Type Validation**:
   ```bash
   scope=$(echo "$result" | jq -r '.scope')
   confidence=$(echo "$result" | jq -r '.confidence')

   # Scope must be valid enum value
   [[ "$scope" =~ ^(research-only|research-and-plan|research-and-revise|full-implementation|debug-only)$ ]] || fail

   # Confidence must be float between 0.0 and 1.0
   echo "$confidence" | grep -Eq '^(0(\.[0-9]+)?|1(\.0+)?)$' || fail
   ```

3. **Confidence Threshold Verification**:
   ```bash
   WORKFLOW_CLASSIFICATION_CONFIDENCE_THRESHOLD=0.8
   result=$(classify_workflow_llm "$description")
   if [ $? -eq 0 ]; then
     # Success - confidence must be >= 0.8
     confidence=$(echo "$result" | jq -r '.confidence')
     awk -v c="$confidence" 'BEGIN {exit (c >= 0.8 ? 0 : 1)}' || fail "Low confidence accepted"
   else
     # Failure - confidence must be < 0.8
     echo "âœ“ Low confidence correctly rejected"
   fi
   ```

#### 4.3 Fallback Behavior Verification

**Scenario**: LLM timeout/error â†’ automatic regex fallback

**Verification Methods**:
1. **Fallback Transparency**:
   ```bash
   # Force timeout
   WORKFLOW_CLASSIFICATION_TIMEOUT=0 result=$(detect_workflow_scope "test" 2>&1)

   # Verify: (1) returns valid scope, (2) no user-visible errors, (3) logs show fallback
   [ -n "$result" ] || fail "Empty result after fallback"
   echo "$result" | grep -vq "ERROR" || fail "User-visible error during fallback"
   echo "$result" | grep -q "method=regex-fallback" || fail "Fallback not logged"
   ```

2. **Fallback Correctness**:
   ```bash
   # Compare hybrid mode with timeout to regex-only mode
   WORKFLOW_CLASSIFICATION_MODE=regex-only regex_result=$(detect_workflow_scope "$description")
   WORKFLOW_CLASSIFICATION_MODE=hybrid WORKFLOW_CLASSIFICATION_TIMEOUT=0 hybrid_result=$(detect_workflow_scope "$description")

   [ "$regex_result" = "$hybrid_result" ] || fail "Fallback produced different result"
   ```

3. **Error State Handling**:
   ```bash
   # Test various error conditions
   for error_condition in timeout invalid_json empty_response missing_fields; do
     result=$(simulate_llm_error "$error_condition" && detect_workflow_scope "$description")
     [ -n "$result" ] || fail "No result returned for error: $error_condition"
     # Verify fallback occurred
     grep -q "regex-fallback" logs.txt || fail "Fallback not triggered for: $error_condition"
   done
   ```

#### 4.4 Performance Verification

**Metrics**: p50/p95/p99 latency, fallback rate, throughput

**Verification Methods**:
1. **Latency Percentile Calculation**:
   ```bash
   # Collect 100 latency samples
   for i in {1..100}; do
     start=$(date +%s%3N)
     detect_workflow_scope "test description $i" >/dev/null
     end=$(date +%s%3N)
     echo "$((end - start))" >> latencies.txt
   done

   # Calculate percentiles
   sort -n latencies.txt | awk '
     {latency[NR] = $1}
     END {
       p50 = latency[int(NR*0.5)]
       p95 = latency[int(NR*0.95)]
       p99 = latency[int(NR*0.99)]
       print "p50:", p50 "ms (target: <300ms)"
       print "p95:", p95 "ms (target: <600ms)"
       print "p99:", p99 "ms (target: <1000ms)"
       exit (p50 > 300 || p95 > 600 || p99 > 1000 ? 1 : 0)
     }
   '
   ```

2. **Fallback Rate Monitoring**:
   ```bash
   # Count LLM success vs fallback
   llm_success=0
   fallback_count=0

   for desc in "${TEST_DESCRIPTIONS[@]}"; do
     DEBUG_SCOPE_DETECTION=1 result=$(detect_workflow_scope "$desc" 2>&1)
     if echo "$result" | grep -q "method=llm"; then
       llm_success=$((llm_success + 1))
     else
       fallback_count=$((fallback_count + 1))
     fi
   done

   fallback_rate=$(echo "scale=2; $fallback_count * 100 / ($llm_success + $fallback_count)" | bc)
   echo "Fallback rate: ${fallback_rate}% (target: <20%)"
   ```

3. **Throughput Testing**:
   ```bash
   # Sequential throughput
   start=$(date +%s)
   for i in {1..50}; do
     detect_workflow_scope "test description $i" >/dev/null
   done
   end=$(date +%s)
   duration=$((end - start))
   throughput=$(echo "50 / $duration" | bc -l)
   echo "Throughput: ${throughput} classifications/sec"
   ```

### 5. Edge Cases and Error Handling Scenarios

#### 5.1 Input Edge Cases

**Category**: Malformed, empty, or extreme inputs

**Test Cases**:
1. **Empty Input**:
   - Test: `detect_workflow_scope ""`
   - Expected: Returns default scope (research-and-plan), logs error
   - Current: PASS (verified in test_scope_detection.sh)

2. **Extremely Long Input** (10,000+ characters):
   - Test: `detect_workflow_scope "$(printf 'research %.0s' {1..2000})"`
   - Expected: Classification succeeds or times out gracefully
   - Current: NOT TESTED

3. **Special Characters**:
   - Test: `detect_workflow_scope "research auth \"patterns\" & create plan's structure <test>"`
   - Expected: JSON escaping works, classification succeeds
   - Current: PASS (verified in test_llm_classifier.sh)

4. **Unicode and Emojis**:
   - Test: `detect_workflow_scope "ç ”ç©¶ authentication patterns ðŸ”"`
   - Expected: UTF-8 handling works, classification succeeds
   - Current: PASS (verified in test_llm_classifier.sh)

5. **Newlines and Control Characters**:
   - Test: `detect_workflow_scope "research\nauth\tpatterns"`
   - Expected: Whitespace normalized, classification succeeds
   - Current: NOT TESTED

#### 5.2 LLM Failure Edge Cases

**Category**: LLM API failures, timeouts, invalid responses

**Test Cases**:
1. **File-Based Signaling Failure**:
   - Test: Request file write fails (disk full, permissions)
   - Expected: Error logged, fallback to regex
   - Current: NOT TESTED

2. **Response File Never Created**:
   - Test: LLM never writes response file (timeout)
   - Expected: Timeout after 10s, fallback to regex
   - Current: PARTIALLY TESTED (WORKFLOW_CLASSIFICATION_TIMEOUT=0)

3. **Response File Corrupted**:
   - Test: Response file contains partial/corrupted JSON
   - Expected: Parse error, fallback to regex
   - Current: NOT TESTED

4. **Concurrent Request Collision**:
   - Test: Multiple workflows run simultaneously (same PID race condition)
   - Expected: Unique file naming ($$) prevents collision
   - Current: NOT TESTED

5. **Temp File Cleanup Failure**:
   - Test: Cleanup trap fails (readonly filesystem)
   - Expected: Warning logged, workflow continues
   - Current: NOT TESTED

#### 5.3 Confidence Threshold Edge Cases

**Category**: Boundary conditions around confidence threshold (default 0.7)

**Test Cases**:
1. **Exact Threshold** (confidence = 0.7):
   - Test: Mock LLM returns confidence 0.7
   - Expected: Accepted (>= threshold)
   - Current: PASS (verified in test_llm_classifier.sh)

2. **Just Below Threshold** (confidence = 0.69):
   - Test: Mock LLM returns confidence 0.69
   - Expected: Rejected, fallback to regex
   - Current: PASS (verified in test_llm_classifier.sh)

3. **Zero Confidence**:
   - Test: Mock LLM returns confidence 0.0
   - Expected: Rejected, fallback to regex
   - Current: PASS (verified in test_llm_classifier.sh)

4. **Maximum Confidence**:
   - Test: Mock LLM returns confidence 1.0
   - Expected: Accepted
   - Current: PASS (verified in test_llm_classifier.sh)

5. **Custom Threshold**:
   - Test: Set WORKFLOW_CLASSIFICATION_CONFIDENCE_THRESHOLD=0.9, mock returns 0.85
   - Expected: Rejected (below custom threshold)
   - Current: PASS (verified in test_llm_classifier.sh)

#### 5.4 Mode Configuration Edge Cases

**Category**: Environment variable edge cases

**Test Cases**:
1. **Invalid Mode String**:
   - Test: `WORKFLOW_CLASSIFICATION_MODE=invalid detect_workflow_scope "test"`
   - Expected: Error logged, returns default scope
   - Current: PASS (verified in test_scope_detection.sh)

2. **Case Sensitivity**:
   - Test: `WORKFLOW_CLASSIFICATION_MODE=HYBRID` (uppercase)
   - Expected: Should work (case-insensitive) OR fail with clear error
   - Current: NOT TESTED

3. **Empty Mode Variable**:
   - Test: `WORKFLOW_CLASSIFICATION_MODE="" detect_workflow_scope "test"`
   - Expected: Uses default (hybrid)
   - Current: NOT TESTED

4. **Unset vs Empty**:
   - Test: `unset WORKFLOW_CLASSIFICATION_MODE; detect_workflow_scope "test"`
   - Expected: Uses default (hybrid)
   - Current: PASS (verified in test_scope_detection.sh)

5. **Mode Switching Mid-Session**:
   - Test: Change WORKFLOW_CLASSIFICATION_MODE between calls
   - Expected: Each call uses current mode value
   - Current: PASS (verified in test_scope_detection.sh)

#### 5.5 Integration Error Scenarios

**Category**: Errors in calling contexts (/coordinate, /supervise, state machine)

**Test Cases**:
1. **State Machine Initialization Failure**:
   - Test: sm_init() calls detect_workflow_scope() with empty description
   - Expected: Default scope returned, state machine initializes with default
   - Current: NOT TESTED

2. **Workflow-Detection.sh Sourcing Failure**:
   - Test: workflow-detection.sh cannot source workflow-scope-detection.sh
   - Expected: Fail-fast error, clear diagnostic message
   - Current: NOT TESTED

3. **Missing CLAUDE_PROJECT_DIR**:
   - Test: Unset CLAUDE_PROJECT_DIR, source workflow-scope-detection.sh
   - Expected: detect-project-dir.sh calculates it, or clear error
   - Current: NOT TESTED (relies on detect-project-dir.sh)

4. **Duplicate Library Sourcing**:
   - Test: Source workflow-scope-detection.sh twice
   - Expected: Source guard prevents duplicate execution
   - Current: NOT TESTED (source guard exists, not verified)

5. **Function Export Failure**:
   - Test: Call detect_workflow_scope from subshell
   - Expected: Function available (exported)
   - Current: NOT TESTED

## Recommendations

### 1. Immediate Priority: Real LLM Integration Testing

**Action**: Create automated LLM integration test framework
**Effort**: 2-3 days
**Impact**: High (validates core functionality)

**Implementation Steps**:
1. Create LLM mock framework using fixtures (`.claude/tests/mocks/mock_llm_classifier.sh`)
2. Update test_llm_classifier.sh to use mocks for deterministic testing
3. Create manual LLM integration test script (`test_llm_integration_manual.sh`)
4. Document manual test procedure in Phase 3 task 3.1

**Acceptance Criteria**:
- 2 skipped tests in test_llm_classifier.sh become automated (7.2, 7.3)
- 3 pending E2E tests become executable with mock framework
- Manual integration test script documents real LLM testing procedure

### 2. High Priority: Performance Benchmarking Suite

**Action**: Create comprehensive performance test suite
**Effort**: 1-2 days
**Impact**: Medium (validates performance targets)

**Implementation Steps**:
1. Create `.claude/tests/bench_workflow_classification.sh` (200 lines)
2. Implement latency percentile measurement (p50/p95/p99)
3. Implement fallback rate monitoring over time
4. Implement throughput testing (sequential and concurrent)
5. Integrate with CI/CD for regression detection

**Acceptance Criteria**:
- p50 latency measured and documented (<300ms target)
- p95 latency measured and documented (<600ms target)
- p99 latency measured and documented (<1000ms target)
- Fallback rate measured (<20% target)
- Throughput baseline established

### 3. Medium Priority: Edge Case Coverage Expansion

**Action**: Add tests for uncovered edge cases
**Effort**: 1 day
**Impact**: Medium (increases robustness)

**Implementation Steps**:
1. Add extremely long input test (10,000+ characters)
2. Add newlines/control characters test
3. Add concurrent request collision test
4. Add temp file cleanup failure test
5. Add case sensitivity test for mode configuration

**Acceptance Criteria**:
- 5 new edge case tests added to test_scope_detection.sh
- All edge cases pass or fail gracefully with clear errors
- No crashes or undefined behavior

### 4. Medium Priority: Integration Testing Enhancement

**Action**: Expand integration testing with real workflows
**Effort**: 1-2 days
**Impact**: Medium (validates end-to-end functionality)

**Implementation Steps**:
1. Create full /coordinate workflow integration test
2. Create full /supervise workflow integration test
3. Test state machine initialization with hybrid classification
4. Test error propagation through workflow stack
5. Verify backward compatibility with existing workflows

**Acceptance Criteria**:
- /coordinate workflow completes successfully with hybrid mode
- /supervise workflow completes successfully with hybrid mode
- State machine integration verified
- Backward compatibility tests pass

### 5. Low Priority: Continuous Monitoring Infrastructure

**Action**: Set up optional production monitoring (Phase 6)
**Effort**: 2-3 days (optional)
**Impact**: Low (quality assurance over time)

**Implementation Steps**:
1. Create classification metrics dashboard (if infrastructure available)
2. Set up alerting rules (fallback rate, error rate, latency)
3. Implement weekly review process
4. Document incident response procedures

**Acceptance Criteria**:
- Metrics dashboard deployed (optional)
- Alerting rules configured (optional)
- Monthly review process scheduled (optional)

### 6. Documentation Recommendations

**Action**: Enhance test documentation
**Effort**: 1 day
**Impact**: Medium (improves maintainability)

**Implementation Steps**:
1. Create comprehensive test README (`.claude/tests/README_hybrid_classification.md`)
2. Document test data fixtures and how to extend them
3. Document manual LLM testing procedure
4. Document performance benchmarking interpretation
5. Add troubleshooting guide for test failures

**Acceptance Criteria**:
- Test README explains all test files and purposes
- Fixtures documentation enables easy dataset expansion
- Manual testing procedure is clear and reproducible
- Performance targets clearly documented

## References

### Implementation Files
- `/home/benjamin/.config/.claude/lib/workflow-llm-classifier.sh` (290 lines) - LLM classifier library
- `/home/benjamin/.config/.claude/lib/workflow-scope-detection.sh` (198 lines) - Unified hybrid classifier
- `/home/benjamin/.config/.claude/specs/670_workflow_classification_improvement/plans/001_hybrid_classification_implementation.md` (1232 lines) - Implementation plan

### Test Files
- `/home/benjamin/.config/.claude/tests/test_llm_classifier.sh` (442 lines, 37 tests, 94.6% pass rate)
- `/home/benjamin/.config/.claude/tests/test_scope_detection.sh` (560 lines, 31 tests, 96.8% pass rate)
- `/home/benjamin/.config/.claude/tests/test_scope_detection_ab.sh` (305 lines, 42 tests, 97.6% pass rate)
- `/home/benjamin/.config/.claude/tests/manual_e2e_hybrid_classification.sh` (196 lines, 6 manual tests)

### Research Reports
- `/home/benjamin/.config/.claude/specs/670_workflow_classification_improvement/reports/001_llm_based_classification_research.md` - LLM classification research
- `/home/benjamin/.config/.claude/specs/670_workflow_classification_improvement/reports/002_comparative_analysis_and_synthesis.md` - Comparative analysis
- `/home/benjamin/.config/.claude/specs/670_workflow_classification_improvement/reports/003_implementation_architecture.md` (74KB) - Architecture design with testing infrastructure section

### Key Architecture Documents
- `/home/benjamin/.config/CLAUDE.md` - Project standards and hierarchical agent architecture
- `/home/benjamin/.config/.claude/docs/concepts/bash-block-execution-model.md` - Subprocess isolation patterns
- `/home/benjamin/.config/.claude/docs/reference/command_architecture_standards.md` - Standards 0, 11, 13, 14
